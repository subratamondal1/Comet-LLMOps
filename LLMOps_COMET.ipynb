{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbEc/EbtZlItrzu1Lz+FHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subratamondal1/Comet-LLMOps/blob/main/LLMOps_COMET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>LLMOps</center>\n",
        "---"
      ],
      "metadata": {
        "id": "Llk1MFObX8tN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1\n",
        "This module will cover:\n",
        "* A brief introduction to LLMs.\n",
        "* Introduction to LLMOps and its main components.\n",
        "* Comparison between LLMOps and MLOps"
      ],
      "metadata": {
        "id": "sbhuhyi4n4x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is LLMOps\n",
        "LLMOps stands for **large language model operations** and refers to the specialized practices and workflows that speed **development**, **deployment** and **management** of AI models throughout their complete lifecycle. LLMOps platforms can deliver:\n",
        "- more efficient library management,\n",
        "- lowering operational costs and\n",
        "- enabling less technical personnel to complete tasks\n",
        "\n",
        "These operations include data preprocessing, language model training, monitoring, fine-tuning and deployment. As with Machine Learning Ops (MLOps), LLMOps is built on a collaboration of data scientists, DevOps engineers and IT professionals."
      ],
      "metadata": {
        "id": "pAx8gzA5mvuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits of LLMOps\n",
        "- **Reduces** time and cost to build and deploy LLMs by streamlining the process.\n",
        "\n",
        "- **Improves** performance and reliability of LLMs through best practices.\n",
        "\n",
        "- **Helps** in effectively managing, scaling and maintaining LLMs.\n",
        "\n",
        "- **Addresses** catastrophic failures via monitoring and maintenance."
      ],
      "metadata": {
        "id": "wom2QIZ4odcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of LLMOps Lifecycle\n",
        "\n",
        "There are six key steps that a Language Model typically goes through:\n",
        "\n",
        "- **Data preparation and exploration**: This step involves collecting, cleaning, and analyzing the data that will be used to train and fine-tune the large language model (LLM). The data should be relevant, diverse, and unbiased to ensure the quality and reliability of the LLM.\n",
        "\n",
        "- **Pretraining LLM**: This step involves training a LLM on a large corpus of text, such as Wikipedia, books, or web pages, using the Transformer architecture. The LLM learns to predict the next word in a sequence, given the previous words, and captures the grammar, semantics, and context of natural language. Pretrained LLMs, such as GPT-4 and BERT, are publicly available and can be used as a starting point for downstream tasks.\n",
        "\n",
        "- **Model fine-tuning and prompt engineering**: This step involves adapting the pretrained LLM to a specific task or domain, such as sentiment analysis, summarization, or question answering, by adding a task-specific layer or head and training it on a smaller dataset of labeled examples. **Prompt engineering** is the process of designing the input and output formats for the LLM, such as adding keywords, prefixes, or suffixes, to elicit the desired response from the LLM.\n",
        "\n",
        "- **Model evaluation and debugging**: This step involves testing the performance and robustness of the fine-tuned LLM on various metrics, such as accuracy, BLEU, or ROUGE, and identifying and fixing any errors, biases, or inconsistencies in the LLM output. This step also involves ensuring the ethical and responsible use of the LLM, by following the guidelines and best practices of Responsible AI.\n",
        "\n",
        "- **Model deployment**: This step involves deploying the fine-tuned LLM to a production environment, where it can interact with users or other systems. This step requires choosing the appropriate infrastructure, platform, and tools to host, serve, and scale the LLM, as well as integrating the LLM with the application logic and user interface.\n",
        "\n",
        "- **Model monitoring and maintenance**: This step involves continuously tracking and improving the performance, reliability, and security of the deployed LLM, by collecting feedback, logs, and metrics, and applying updates, patches, or retraining as needed. This step also involves ensuring the compliance and governance of the LLM, by following the regulations and standards of the domain or industry."
      ],
      "metadata": {
        "id": "CQn3DtZQp2m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMOps Landscape\n",
        "1. **LLM Providers**\n",
        "  * OpenAI\n",
        "  * Cohere\n",
        "  * Anthropic\n",
        "\n",
        "2. **LLM Tools**\n",
        "  * Pinecone\n",
        "  * Comet\n",
        "  * MLflow\n",
        "  * Snorkel\n",
        "  * W&B\n",
        "\n",
        "3. **LLM Frameworks**\n",
        "  * Hugging Face\n",
        "  * Langchain\n",
        "\n",
        "4. **LLM Infrastructure**\n",
        "  * Databricks\n",
        "  * Snowflake\n",
        "  * AzureAI"
      ],
      "metadata": {
        "id": "DLZtEC_3sEpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMOps vs MLOps\n",
        "* MLOps is broad.\n",
        "* LLMOps focuses on LLMs.\n",
        "* LLMOps requires dealing with different:\n",
        "  * data requirements\n",
        "  * experimentations\n",
        "  * evaluation strategies\n",
        "  * costs and latency"
      ],
      "metadata": {
        "id": "gabyYm9AUp8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some LLMOps considerations\n",
        "* Tuning base models which requires high-quality data and efficient techniques\n",
        "* Evaluating and debugging LLMs efficiently which depends on the task and business objective\n",
        "* Improving results via prompt engineering and efficient prompt management, tracking and optimization\n",
        "* Building pipelines that require complex chaining of LLM calls and operations\n",
        "* Optimizing for cost and latency\n",
        "* Versioning and maintaining models to address safety, cost and quality.\n",
        "* Collecting human feedback to improve LLMs."
      ],
      "metadata": {
        "id": "4rQZPalQW-tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMET ML\n",
        "\n",
        "In this course we will leverage Comet ML experiment management and LLMOps tools, including integrations.\n",
        "\n",
        "* **Comet ML** supports instrumenting and finetuning LLMs.\n",
        "* **Comet LLM** provides prompt engineering tools to track and debug LLMs."
      ],
      "metadata": {
        "id": "2Ry-MnMiYGYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2\n",
        "This module will cover:\n",
        "* Challenges and strategies for training LLMs.\n",
        "* How to select LLMs?\n",
        "* Prompt engineering techniques for working with LLMs.\n",
        "* Finetuning and Tracking LLMs.\n",
        "* Evaluating LLMs."
      ],
      "metadata": {
        "id": "BG-gjHDBiVfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training LLMs: Strategies and Challenges\n",
        "**Key considerations for training your own LLMs:**\n",
        "\n",
        "* Enables customization but increases training efforts\n",
        "* Requires large high-quality data\n",
        "* Lots of computational resources\n",
        "* Requires advanced tooling to operationalize\n",
        "* Saves on costs and improves latency\n",
        "* Improves privacy, safety and inference speed\n",
        "\n",
        "> **LLMOps is crucial to get this right.**"
      ],
      "metadata": {
        "id": "0uD2dXiNBI_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting your right LLM\n",
        "\n",
        "**Selecting the right LLM model will depend on:**\n",
        "* Cost, speed and quality\n",
        "* Model size\n",
        "* Training data\n",
        "* Application\n",
        "\n",
        "**Other key considerations:**\n",
        "* Optimizing for convenience allows using LLM providers (e.g ChatGPT or Claude)\n",
        "* Customizability may require using open-source base models\n",
        "* Privacy may require training internal custom models and using your own data."
      ],
      "metadata": {
        "id": "ABX7Oa2cCj1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Models\n",
        "**Quality**\n",
        "* ChatGPT\n",
        "* Claude\n",
        "* command-xlarge\n",
        "* Llama 2\n",
        "\n",
        "**Open Source**\n",
        "* Llama 2\n",
        "* T5/Flan-T5\n",
        "* Pythia\n",
        "\n",
        "**Speed**\n",
        "* GPT-3.5\n",
        "* Claude-instant\n",
        "* command-medium"
      ],
      "metadata": {
        "id": "cwvKFe08EkCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering with LLMs"
      ],
      "metadata": {
        "id": "q3CxNRxyFjS0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghdif7wwYgEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJnLbGqBl7jz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI4JnSHwnJL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}